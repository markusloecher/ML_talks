%% $Id: screening2.tex 42 2012-08-30 12:41:02Z natalie $
%% $HeadURL: https://server3.subversion-server.com/natp/svn/screening2/_slides/screening2.tex $
%% $Date: 2012-08-30 14:41:02 +0200 (Thu, 30 Aug 2012) $
%% $Revision: 42 $
%% $Author: natalie $

%\documentclass[xcolor=table,notes=show,10pt]{beamer}
\documentclass[xcolor=table,10pt]{beamer}
%\documentclass[xcolor=table,handout,10pt]{beamer}
%\PassOptionsToPackage{table}{xcolor}
\usepackage{beamerthemeMalmoe}  % Clean, less color, easy on printing
%\usepackage{beamerthemeboxes}
\definecolor{fsblue}{RGB}{0, 72, 128}
\definecolor{fsblue}{RGB}{54,66,109}
\definecolor{lightgrey}{RGB}{245,245,245}
\definecolor{mygrey}{RGB}{230,230,230}
\setbeamercolor{author in head/foot}{bg=fsblue}
%\setbeamercolor{title in head/foot}{bg=fsblue}
\setbeamercolor{title in head/foot}{bg=white,fg=darkblue}
\setbeamercolor{author in head/foot}{bg=white,fg=darkblue}
\setbeamerfont{frametitle}{size=\large,series=\bfseries}
\setbeamercolor{frametitle}{fg=darkred}

\setbeamerfont{block title}{size=\normalsize,series=\bfseries}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamercolor{block body}{bg=lightgrey}
\setbeamercolor{block title}{bg=mygrey,fg=black}

%\setbeamersize{text margin left=1.5cm,text margin right=1.5cm} 
\setbeamerfont*{itemize/enumerate subbody}{parent=itemize/enumerate body}
\setbeamerfont*{itemize/enumerate subsubbody}{parent=itemize/enumerate
  body}

\usepackage[absolute,overlay]{textpos}
\textblockorigin{10mm}{3mm}
\newenvironment{reference}[2]{% 
  \begin{textblock*}{\textwidth}(#1,#2) 
      \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}
} 

\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{mindmap}
\usetikzlibrary{arrows,snakes,tikzmark}

\tikzset{
  every overlay node/.style={
    anchor=north west,
  },
}
\def\tikzoverlay{%
   \tikz[baseline,overlay]\node[every overlay node]
}%


\usepackage{setspace}
%\setstretch{1.25}
\setstretch{1.15}
\usepackage{slashbox}
\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{fancybox}
\usepackage{amsmath,amsthm,bm}
%\usepackage[longnamesfirst]{natbib}
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}

\usepackage[makestderr]{pythontex}

\definecolor{markergreen}{rgb}{0.6, 1.0, 0}
\definecolor{darkred}{rgb}{.7,0,0}
\providecommand{\marker}[1]{\fcolorbox{markergreen}{markergreen}{{#1}}}
\providecommand{\natp}[1]{\textcolor{darkred}{#1}}

\title[BVS, 17 Dec 2019]{\bigskip\\
Bayesian Linear Models\\ and\\ Bayesian Variable Selection\\[3pt] 
}
\author[{\textcopyright} N.\ Packham]{
      Natalie Packham\\
      Berlin  School of Economics \& Law
  }
\institute{\normalsize
 Bayesian Statistics Explorations
}

\date{
17 December 2019
}
\logo{
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Reset some default colors for itemize/enumerate/description environments
%
\setbeamercolor{description item}{fg=darkred!80!black}  %  Color of key word in desciption 
%\setbeamercolor{alerted text}{fg=darkred!80!black}  %  Color of key word in desciption 
\setbeamercolor{alerted text}{fg=darkblue}  %  Color of key word in desciption 
%
\definecolor{darkred}{rgb}{0.7,0,0}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\setbeamercolor{item}{fg=darkgreen}  %  The dot color
\definecolor{darkblue}{rgb}{0,0,0.8}
\setbeamercolor{itemize/enumerate body}{fg=black}    % Text Level 1
%\setbeamercolor{itemize/enumerate subbody}{fg=darkblue}    % Text Level 2
\setbeamercolor{itemize/enumerate subsubbody}{fg=green!25!black}    % Text Level 3

\setbeamertemplate{headline}{}

\definecolor{grey}{RGB}{.25,.0,.25}
%\definecolor{fsblue}{rgb}{0, 0, 1}

\setbeamerfont{note page}{size=\footnotesize}
\setbeamercolor{math text}{fg=darkblue}
\setbeamercolor{math text displayed}{fg=darkblue}

\providecommand{\mgreenbox}[1]{\fcolorbox{green}{green}{$#1$}}
\providecommand{\mredbox}[1]{\fcolorbox{red}{white}{$#1$}}
\providecommand{\mredblockbox}[1]{\fcolorbox{red}{blocktitle.bg!.0!bg}{$#1$}}
\providecommand{\mbluebox}[1]{\fcolorbox{blue}{white}{$#1$}}
\providecommand{\mwhitebox}[1]{\fcolorbox{white}{white}{$#1$}}
\providecommand{\hyperl}[2]{\textcolor{blue}{\hyperlink{#1}{#2}}}
\providecommand{\Nzero}{\mathbb N_0}

\providecommand{\bluebox}[1]{\text{\fcolorbox{blue!10}{blue!10}{#1}}}  
\providecommand{\redbox}[1]{\text{\fcolorbox{red!15}{red!15}{#1}}}  
\providecommand{\greenbox}[1]{\text{\fcolorbox{darkgreen!15}{darkgreen!15}{#1}}}  

\providecommand{\limc}{\ensuremath{\lim_{C\rightarrow-\infty}}}
\providecommand{\limcp}{\ensuremath{\lim_{C\rightarrow\infty}}}
\providecommand{\ct}{\ensuremath{\cos \theta}}
\providecommand{\st}{\ensuremath{\sin \theta}}
\providecommand{\cp}{\ensuremath{\cos \varphi}}
\renewcommand{\sp}{\ensuremath{\sin \varphi}}
%\usepackage{sfmath}
\usepackage{multirow}

\newtheorem{proposition}[theorem]{Proposition}

\providecommand{\newblock}{}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
  \insertshorttitle\hfill%
  \insertframenumber% \,/\,\inserttotalframenumber
}

\AtBeginSection[]
{
%  \begin{frame}[shrink]
%  \begin{frame}[squeeze]
  \begin{frame}
    \frametitle{Contents}
%      {\small \tableofcontents[currentsubsection,hideothersubsections]}
      {\small \tableofcontents[currentsubsection]}
  \end{frame}
}

\AtBeginSubsection[]
{
  \begin{frame}
    \frametitle{Contents}
      % {\small \tableofcontents[currentsubsection,hideothersubsections,subsectionstyle=show/shaded/hide]}
      {\small \tableofcontents[currentsubsection,subsectionstyle=show/shaded]}
  \end{frame}
}

\input{definitions}
\providecommand{\x}{\ensuremath{\mathbf{x}}}%
\providecommand{\y}{\ensuremath{\mathbf{y}}}%
\providecommand{\Eref}[1]{Equation~(\ref{#1})} %%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\setcounter{tocdepth}{2}%

\frame{\thispagestyle{empty}\titlepage}

  \logo{}

% \begin{frame} \frametitle{Contents}
% {\small \tableofcontents}
% \end{frame}
%%


  
\section{Overview}

\begin{frame}
  \frametitle{Overview}
  \begin{itemize}
  \item \alert{Bayesian linear models}: Bayesian approach to coefficient
    estimation in linear models
  \item \alert{Bayesian variable selection}: Instead of assiging
    non-zero coefficients to all independent variables, \alert{select}
    the most relevant variables
  \item Special cases: \alert{Ridge Regression} ($L2$-regularisation),
    \alert{Lasso} ($L1$-regularisation) 
  \item Exposition here uses material mainly from \citep{Fahrmeir2009}
    and \citep{Koop2003}. 
  \end{itemize}
\end{frame}


\section{Bayesian linear models}
% \section{Bayesian inference with conjugate prior}

\begin{frame}
  \frametitle{Bayesian linear models}
  \begin{itemize}
  \item We consider the linear model
\begin{equation*}
  \bm y = \bm X \bm \beta + \bm\varepsilon.
\end{equation*}
\vspace*{-\baselineskip}
\item Under the classical (strong) assumptions,
\begin{gather*}
  \bm y\sim \Ncdf(\bm X\bm \beta, \sigma^2\bm I)\\
  \hat{\bm\beta} \sim \Ncdf(\bm \beta, \sigma^2 (\bm
  X'\bm X)^{-1})
\end{gather*}
\vspace*{-\baselineskip}
\item In a Bayesian setting -- see Section 3.5 of \citep{Fahrmeir2009}
  --  we assume
\begin{equation*}
  \bm y|\bm \beta, \sigma^2\sim \Ncdf(\bm X\bm \beta,
  \sigma^2\bm I),
\end{equation*}
with $\bm\beta$ and $\sigma^2$ stochastic.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Conjugate prior}
  \begin{itemize}
\item A conjugate prior is 
\begin{align*}
  \bm \beta|\sigma^2&\sim \Ncdf(\bm m, \sigma^2 \bm M)\\
  \sigma^2 &\sim \text{IG}(a,b),
\end{align*}
where $\text{IG}(a,b)$ denotes the inverse gamma distribution with
parameters $a, b$.\footnotemark[1]
\begin{reference}{0mm}{64mm}
Let $Y$ follow a Gamma distribution with
  parameters $a,b$, i.e., $Y\sim G(a,b)$. Then $X=1/Y$ is {\em inverse
  gamma} distributed, $X\sim \text{IG}(a,b)$. The density of $X$ is
  \begin{equation*}
    f(x) = \frac{b^a}{\Gamma(a)} x^{-(a+1)} \exp(-b/x), \quad x>0. 
  \end{equation*}
\end{reference}
\vspace*{3\baselineskip}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Conjugate prior}
  \begin{itemize}
\item Alternative formulation: pair $(\bm\beta, \sigma^2)$
follows \alert{NIG distribution (NIG = normal inverse gamma)} with 
parameters $(\bm m, \bm M, a, b)$.\footnotemark[2]
\vspace*{6\baselineskip}
\begin{reference}{0mm}{40mm}
  The random variables $(\bm \beta, \sigma^2)$ follow a {\em normal
    inverse gamma distribution\/} with parameters $\bm m, \bm M, a, b$
  if 
  \begin{align*}
    \bm \beta|\sigma^2, \bm m, \bm M &\sim \Ncdf(\bm m,
                                         \sigma^2\bm M)\\
    \sigma^2|a, b &\sim \text{IG}(a,b).
  \end{align*}
  The density of $(\bm \beta, \sigma^2)\sim \text{NIG}(\bm m, \bm M,
  a, b)$ is
  \begin{multline*}
    p(\bm \beta,\sigma^2) = p(\bm\beta |\sigma^2) p(\sigma^2) \\
    = \frac{1}{(2\pi)^{p/2} (\sigma^2)^{p/2} \left|\bm
      M\right|^{1/2}} \exp\left( -\frac{1}{2\sigma^2} (\bm \beta-\bm
      m)'\bm M^{-1} (\bm \beta-\bm m)\right) %
      \frac{b^a}{\Gamma(a)} \frac{1}{(\sigma^2)^{a+1}}
      \exp\left(-\frac{b}{\sigma^2}\right). 
  \end{multline*}
\end{reference}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Marginal and posterior distributions}
  \begin{itemize}
  \item The marginal distribution of $\bm\beta$ is a multivariate 
    $t$-distribution: 
    \begin{equation*}
      \bm\beta \sim t_{2a} \left(\bm m, \frac{b}{a} \bm M\right). 
    \end{equation*}
    \vspace*{-\baselineskip}
  \item The {\em posterior distributions\/} are given as
    \begin{align*}
      \bm\beta|\cdot &\sim \Ncdf\left(\bm{\mu}_\beta,
                       \bm{\Sigma}_\beta\right)\\ 
      \sigma^2|\cdot &\sim \text{IG}(a', b')\\
      (\bm\beta, \sigma^2)|\bm y&\sim \text{NIG}(\tilde{\bm m},
                                  \tilde{\bm M}, \tilde a, \tilde b),
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Marginal and posterior distributions}
  \begin{itemize}
  \item where
    {\footnotesize %
      \begin{align*}
        \bm{\Sigma}_\beta &= \left(\frac{1}{\sigma^2} \bm X'\bm X
                            + \frac{1}{\sigma^2} \bm M^{-1}\right)\\
        \bm{\mu}_\beta &= \bm{\Sigma}_\beta \left(\frac{1}{\sigma^2}
                         \bm X' \bm y + \frac{1}{\sigma^2} \bm
                         M^{-1} \bm m\right)\\
        a' &= a + \frac{n}{2} + \frac{p}{2}\\
        b' &= b + \frac{1}{2} \left(\bm y-\bm X\bm\beta'\right) \left(\bm
             y-\bm X\bm \beta\right) + \frac{1}{2} \left(\bm \beta-\bm
             m\right)' \bm M^{-1} \left(\bm \beta - \bm m\right)\\
        \tilde {\bm M} &= \left(\bm X'\bm X + \bm M^{-1}\right)^{-1}\\
        \tilde {\bm m} &= \tilde {\bm M} \left(\bm M^{-1} \bm m +
                         \bm X'\bm y\right)\\
        \tilde a &= a + \frac{n}{2}\\
        \tilde b &= b + \frac{1}{2}\left(\bm y'\bm y + \bm m'\bm
                   M^{-1}\bm m - \tilde{\bm m}'\tilde{\bm
                   M}^{-1} \tilde{\bm m}\right). 
      \end{align*}
    }
    \vspace*{-\baselineskip}
  \item To sample, one can use a \alert{Gibbs sampler} that draws
    iteratively from $\bm{\beta}|\cdot$ and $\sigma^2|\cdot$.
  \end{itemize}
\end{frame}

  \begin{frame}
  \frametitle{Inference}
  \begin{itemize}
\item A point estimate for $\bm\beta$ is given as
\begin{equation*}
  \hat{\bm\beta}^B = \E(\bm\beta|\bm y) = \left(\bm X'\bm X + \bm
    M^{-1}\right)^{-1}  \left(\bm M^{-1} \bm m + \bm X' \bm y\right). 
\end{equation*}
\vspace*{-\baselineskip}
\item Writing $\bm A = (\bm X'\bm X + \bm M^{-1})^{-1} \bm X'\bm X$, one
kann write the Bayesian point estimate as a weighted average of the
prior expectation $\bm m$ and the OLS estimate $\hat{\bm \beta}$:
\begin{equation*}
  \hat{\bm\beta}^B = (\bm I-\bm A)\bm m + \bm A \hat{\bm \beta}. 
\end{equation*}
\end{itemize}
\end{frame}

% \section{Examples}

% \subsection{Linear regression model simulation}

% \begin{frame}[fragile]
% \frametitle{Linear regression model simulation}
% \begin{itemize}
% \item In the first example, we simulate a linear regression model and
%   perform OLS:
% \end{itemize}
% {\scriptsize%
% \begin{pyconsole}[random][frame=single]
% import numpy as np
% import scipy as sp
% import pandas as pd
% import scipy.stats as scs
% import statsmodels.api as sm
% import math
% import matplotlib.pyplot as plt

% # correlation matrix
% c = np.linalg.cholesky([[0.2, 0.2, 0], [0.2, 1, 0], [0, 0, 1.0]])

% np.random.seed(1236)
% n=200;
% Z = np.random.normal(size=(n,3))
% X = pd.DataFrame(c.dot(Z.transpose())).transpose()
% X = sm.add_constant(X)
% beta = [0.0, 0, 0.5, 0.2]
% Y = X.dot(beta)
% X = X.drop(columns=[2]) # last column is the error term
% ols = sm.OLS(Y, X)
% result=ols.fit()
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
% \frametitle{Linear regression model simulation}
% {\scriptsize%
% \begin{pyconsole}[random][frame=single]
% print(result.summary())
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
% \frametitle{Linear regression model simulation}
% {\scriptsize%
% \begin{pyconsole}[random][frame=single]
% results = pd.DataFrame(result.params.round(4), columns=['OLS coef'])
% results['OLS pval'] = result.pvalues.round(4)
% results['CI1'] = result.conf_int()[0].round(4)
% results['CI2'] = result.conf_int()[1].round(4)
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{Linear regression model simulation}
%   \begin{itemize}
% \item Next, we set up the posterior distributions; the choices of $\nu,
% \lambda$ correspond to a relative non-informative prior, see
% \citep{Koop2003}:
% \end{itemize}

% {\scriptsize%
% \begin{pyconsole}[random][frame=single]
% n, p = X.shape
% s = 10 # relatively non-informative prior
% lam = result.resid.var()
% nu = 25 
% a = nu/2
% b = nu * lam / 2    
% M = s**2 * sp.identity(p)
% Minv = sp.linalg.inv(M)
% m = np.zeros(p)
% tM = sp.linalg.inv((X.transpose()).dot(X) + Minv)
% tMinv = (X.transpose().dot(X) + Minv)
% tm = tM.dot(Minv.dot(m) + (X.transpose()).dot(Y))
% ta = a + n/2
% tb = b + 0.5 * ((Y.transpose()).dot(Y) \
%                 + (m.transpose()).dot(Minv.dot(m)) \
%                 - (tm.transpose()).dot(tMinv.dot(tm)))
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{Linear regression model simulation}
%   \begin{itemize}
%   \item Compare the OLS results with the Bayesian results

%     (hpd = highest posterior density):
% \end{itemize}
% {\scriptsize%
%   \begin{pyconsole}[random][frame=single]
% results['bayes_mean'] = tm.round(4)
% results['hpd_2.5'] = [scs.t.ppf(0.025, 2*ta, loc = tm[k], \
%       scale = np.sqrt(((tb/ta) * tM)[k,k])).round(4) for k in range(len(tm))]
% results['hpd_97.5'] = [scs.t.ppf(0.975, 2*ta, loc = tm[k], \
%       scale = np.sqrt(((tb/ta) * tM)[k,k])).round(4) for k in range(len(tm))]

% results
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{Linear regression model simulation}
%   \begin{itemize}
% \item Plot the posterior densities of $\bm\beta$:
% \end{itemize}
% {\scriptsize%
%   \begin{pyconsole}[random][frame=single]
% plt.clf()
% for k in range(p):
%     loc = tm[k]
%     scale = np.sqrt(((tb/ta) * tM)[k,k])
%     x = np.arange(loc-4*scale,loc+4*scale,0.001)
%     _ = plt.plot(x, scs.t.pdf(x, 2*ta, loc = loc, scale = scale), lw=2);

% plt.savefig('posterior1.pdf')
% \end{pyconsole}
% }
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{Linear regression model simulation}
%   \begin{itemize}
%   \item Output produced:
%   \end{itemize}
%   \begin{center}
%   \includegraphics[scale=.5]{posterior1.pdf}
% \end{center}
% \end{frame}

\subsection{VW example}

\begin{frame}
  \frametitle{Factor model}
  \begin{itemize}
  \item Typical problem in finance applications:
    \begin{center}
      \alert{estimate dependencies of individual stocks}
    \end{center}
  \item Direct correlation estimator too noisy
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{VW example}
%   \begin{itemize}
  \item Use \alert{(linear) factor model}; express a vector of returns
    $(r_1, \ldots, r_p)$ as
    \begin{equation*}
      r_i=\alpha_i + \beta_{i1} F_1 + \beta_{i2} F_2 + \cdots + \beta_{id}
      F_d + \varepsilon_i, \qquad i=1,\ldots, p,
    \end{equation*}
    with
    \begin{itemize}
      \addtolength{\itemsep}{2pt}
    \item $F_1, \ldots, F_d$ the {\bf common factor (returns)},
    \item $\beta_{i1}, \beta_{i2}, \ldots, \beta_{id}$ {\bf
        factor loadings}.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Factor model}
  \begin{itemize}
  \item The $p \times p$ covariance matrix of the returns $(r_1,
    \ldots, r_p)$ is given by
    \begin{equation*}
      \Sigma \approx B\, \Omega\, B^T,
    \end{equation*}
    where 
    \begin{itemize}
      \addtolength{\itemsep}{3pt}
    \item $B$ denotes $p\times d$ matrix of factor loadings,
    \item $\Omega$ denotes the $d\times d$ covariance matrix of
      the common factors.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{VW example}
  \begin{itemize}
  \item In this example, we model VW stock returns as a linear
    function of \alert{MSCI (GICS) industry} and \alert{MSCI country
      factors}. 
  \item The data set consists of daily returns of
    \begin{itemize}
    \item MSCI stock indices representing 11 industries and 24
      countries;
    \item individual stock returns
    \end{itemize}
  \item Time period: 2002-2018
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{VW example}
{\scriptsize%
  \begin{pyconsole}[vw][frame=single]
import numpy as np
import scipy as sp
import pandas as pd
import scipy.stats as scs
import statsmodels.api as sm
import math
import matplotlib.pyplot as plt

data = pd.read_csv('returns.csv', index_col=0)

Y = data['VOW3 GY Equity']
X = data[data.columns[0:34]]
X = sm.add_constant(X)
n, p = X.shape

ols = sm.OLS(Y, X)
result=ols.fit()
#print(result.summary()) # results will be shown together with Bayes' analysis

results = pd.DataFrame(result.params.round(4), columns=['OLS coef'])
results['OLS pval'] = result.pvalues.round(4)
\end{pyconsole}
}
\end{frame}


\begin{frame}[fragile]
  \frametitle{VW example}
  \begin{itemize}
  \item Setting up the posterior distributions:
  \end{itemize}
  {\scriptsize%
  \begin{pyconsole}[vw][frame=single]
n, p = X.shape
s = 10 # relatively non-informative prior
lam = result.resid.var()
nu = 25
a = nu/2
b = nu * lam / 2
M = s**2 * sp.identity(p)
Minv = sp.linalg.inv(M)
m = np.zeros(p)
tM = sp.linalg.inv((X.transpose()).dot(X) + Minv)
tMinv = (X.transpose().dot(X) + Minv)
tm = tM.dot(Minv.dot(m) + (X.transpose()).dot(Y))
ta = a + n/2
tb = b + 0.5 * ((Y.transpose()).dot(Y) \
                + (m.transpose()).dot(Minv.dot(m)) \
                - (tm.transpose()).dot(tMinv.dot(tm)))
\end{pyconsole}
}
\end{frame}


\begin{frame}[fragile]
  \frametitle{VW example}
  \begin{itemize}
  \item Compare the OLS results with the Bayesian results (hpd =
    highest posterior density):
  \end{itemize}
  {\scriptsize%
\begin{pyconsole}[vw][frame=single]
results['bayes_mean'] = tm.round(4)
results['hpd_2.5'] = [scs.t.ppf(0.025, 2*ta, loc = tm[k], \
      scale = np.sqrt(((tb/ta) * tM)[k,k])).round(4) for k in range(len(tm))]
results['hpd_97.5'] = [scs.t.ppf(0.975, 2*ta, loc = tm[k], \
      scale = np.sqrt(((tb/ta) * tM)[k,k])).round(4) for k in range(len(tm))]
\end{pyconsole}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{VW example}
{\scriptsize %
\begin{pyconsole}[vw][frame=single]
print(results[:10])
print(results[-10:])
\end{pyconsole}
}
% \begin{tikzpicture}
% \node [anchor=west] (note) at (-1,3) {\Large Note};
% \node [anchor=west] (water) at (-1,1) {\Large Water};
% \begin{scope}[xshift=1.5cm]
%     % \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.7\textwidth]{../images/EiffelWide.jpg}};
%     \begin{scope}[x={(image.south east)},y={(image.north west)}]
%         \draw[red,ultra thick,rounded corners] (0.48,0.80) rectangle (0.55,0.95);
%         \draw [-latex, ultra thick, red] (note) to[out=0, in=-120] (0.48,0.80);
%         \draw [-stealth, line width=5pt, cyan] (water) -- ++(0.4,0.0);
%     \end{scope}
% \end{scope}
% \end{tikzpicture}%
 \tikzoverlay[text width=.375\linewidth] at
  (-.02\linewidth,.125\textheight) { \tikz node (label) at (0,0)[]{
        \draw[red,ultra thick,rounded corners] (0.48,0.80) rectangle
        (8.5,1.2);
      % \includegraphics[scale=.3]{_pics/partition.pdf}%
    }; };
 \tikzoverlay[text width=.375\linewidth] at
  (-.02\linewidth,.725\textheight) { \tikz node (label) at (0,0)[]{
        \draw[red,ultra thick,rounded corners] (0.48,0.80) rectangle
        (9,1.2);
      % \includegraphics[scale=.3]{_pics/partition.pdf}%
    }; };
\end{frame}

\section{Bayesian variable selection}

\begin{frame}
  \frametitle{Bayesian variable selection}
  \begin{itemize}
  \item \alert{Bayesian variable selection (BVS)} refers to Bayesian
    methods of choosing the variables to include in a model.
  \item We consider two methods:
    \item \alert{Bayesian model selection} compares \alert{posterior
        probabilities} of different models.
  \item \alert{Spike and slab priors} include an indicator variable
    for each coefficient and determines the indicator variable's
    \alert{posterior probability} of taking value one.
  \item References are \citep{Koop2003,Fahrmeir2009,Fahrmeir2013}.
  \end{itemize}
\end{frame}

\subsection{Bayesian model comparison}

\begin{frame}
  \frametitle{Bayesian model comparison}
  \begin{itemize}
  \item Denote candidate models by $M_i$, $i=1,\ldots, m$.
  \item For example in a linear regression setting, each model $M_i$
    includes a specific subset of independent variables and excludes
    the other variables.
  \item The \alert{posterior model probability} is $p(M_i|\bm y)$.
  \item Using Bayes rule:
    \begin{equation}
      \label{eq:2}
      p(M_i|\bm y) = \frac{p(\bm y|M_i) p(M_i)} {p(\bm y)},
    \end{equation}
    where
    \begin{itemize}
      \addtolength{\itemsep}{3pt}
    \item $p(M_i)$ is the \alert{prior model probability}
    \item $p(\bm y|M_i)$ is called the \alert{marginal likelihood};
    \item $p(\bm y) = \sum_i p(\bm y|M_i) p(M_i)$. 
    \end{itemize}
    (see e.g.\ Appendix B.5.4 of \citep{Fahrmeir2013})
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian model comparison}
  \begin{itemize}
  \item MCMC sampling makes use of
    $p(M_i|\bm y)\propto p(\bm y|M_i) p(M_i)$.
  \item A popular model prior (Section 4.4.3 of \citep{Fahrmeir2013})
    is
    \begin{equation}
      \label{eq:7}
      p(M_i) = \theta^{p_i} (1-\theta)^{p-p_i},
    \end{equation}
    where
    \begin{itemize}
      \addtolength{\itemsep}{2pt}
    \item $\theta\in (0,1)$,
    \item $p_i$ is the number of independent variables (equivalently,
      the number of coefficients to be estimated) in model $M_i$,
    \item $p$ is the full number of variables / coefficients.
    \end{itemize}
  \item If $\theta=1/2$ (uninformative), $p(M_i)$ is equal for all
    models, in which case
    \begin{equation*}
      p(M_i|\bm y)\propto p(\bm y|M_i).
    \end{equation*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian model comparison$^\ast$}
  \begin{itemize}
  \item Other choices of $\theta$ are motivated as follows:
  \item Let $S$ denote the (unknown) model size.
  \item Define indicator variables $\gamma_k$, $k=1,\ldots, p$, with
    $\gamma_k=1$ if the coefficient $\beta_k$ is included in the model
    and zero otherwise (i.e., $\gamma_k=1$ iff $\beta_k\not=0$).
  \item Then:
    \begin{itemize}
      \addtolength{\itemsep}{2pt}
    \item each $\gamma_k$ follows a Bernoulli distribution,
      $\gamma_k\sim B(1,\theta)$,
    \item $S$ follows a binomial distribution, $S\sim B(p,\theta)$

      (assuming independence of $\gamma_1, \ldots, \gamma_p$).
    \end{itemize}
  \item Hence, $\E(S) = \theta\cdot p$.
  \item A natural way to specify $\theta$ is to choose the expected
    model size and set $\theta = \E(S) / p$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian model comparison}
  \begin{itemize}
  \item Given that posterior density for model $M_i$ is
    \begin{equation*}
      p(\bm \beta, \sigma^2|\bm y,M_i) = \frac{p(\bm y|\bm\beta,
        \sigma^2,M_i) p(\bm\beta, \sigma^2|M_i)} {p(\bm y|M_i)}, 
    \end{equation*}
    re-arranging gives the marginal likelihood as
    \begin{equation*}
      p(\bm y|M_i) = \frac{p(\bm y|\bm\beta,
        \sigma^2,M_i) p(\bm\beta, \sigma^2|M_i)} { p(\bm \beta,
        \sigma^2|\bm y,M_i)}, 
    \end{equation*}
    which consists of likelihood, prior and posterior under $M_i$.
  \item A closed formula exists for the NIG case above (see e.g.\
    \citep{Koop2003}):
    \begin{equation}
      \label{eq:1}
      p(\bm y|M) = \frac{1}{(2\pi)^{n/2}} \sqrt{\frac{\text{det}(\tilde
          {\bm M})} {\text{det}(\bm M)}} \frac{b^a}{{\tilde b}^{\tilde
          a}} \frac{\Gamma(\tilde a)} {\Gamma(a)}. 
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Marginal likelihood in VW example}
{\scriptsize%
\begin{pyconsole}[vw][frame=single]
from python import BVS
# BVS is my class for Bayesian Variable Selection

bm = BVS.BayesModel(np.array(Y), np.array(X), m, 1 * M, a, b)


# The call below produces an overflow error; this happens a lot as
# often very large and very small constants are involved
bm.marginal_likelihood()
\end{pyconsole}
}
\begin{itemize}
\item We'll use MCMC later to compute this quantity.
\item It also helps to take logs...
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian model comparison}
  \begin{itemize}
  \item Define indicator variables $\gamma_k$, $k=1,\ldots, p$, with
    $\gamma_k=1$ if the coefficient $\beta_k$ is included in the model
    and zero otherwise (i.e., $\gamma_k=1$ iff $\beta_k\not=0$).
  \item Posteriori probability of $\gamma_k$ across all models
    $M_{\gamma}$ given by \alert{posterior inclusion probabilities
      (PIP)}:
    \begin{equation}
      \label{eq:3}
      \p(\gamma_k=1|\bm y) = \sum_{\beta_k\in M_\gamma} \p(M_\gamma|\bm y).
    \end{equation}
    \vspace*{-\baselineskip}
  \item If number of parameters $p$ is large, then full calculation of
    $2^p$ posterior model probabilities is infeasible.
  \item $\Rightarrow$ Use Monte Carlo simulation or MCMC.
  \item PIP's are determined as the \alert{frequency} of visited models
    including the covariate relative to the total number of visited
    models (see page 245 of \citep{Fahrmeir2013}).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (I)$^\ast$}
  \begin{itemize}
  \item \citep{Fahrmeir2013}, pp.\ 250, demonstrate that, instead of
    choosing a fixed prior parameter $\theta$, a Beta-distributed
    hyper prior leads to a more uninformative prior.
  \item Idea is that parameter $\theta$ itself is modelled by
    a $\text{Beta}(a,b)$ distribution.
  \item This leads to modification of the prior model probability,
    cf.\ \eqref{eq:7}: 
    \begin{equation}
      \label{eq:8}
      p(M_i) = \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \frac{\Gamma(a+p_i)
        \Gamma(b+p-p_i)} {\Gamma(a+b+p)}. 
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (I)$^\ast$}
  \begin{itemize}
\item The two approaches can be compared through the expectation of
    $S$, the model size, where
    \begin{align*}
      \E(S) &= \theta\cdot p&\text{ (fixed prior)}\\
      \E(S) &= \frac{a}{a+b}\cdot p&\text{ (Beta hyper prior)}.
    \end{align*}
  \item Choosing $a=1$ one can infer $b=(p-\E S)/(\E S)$.
  \item This approach is equal to a simple \alert{spike and slab
      prior}, see \citep{Fahrmeir2013}, p.\ 253.
  \end{itemize}
\end{frame}


\subsection{Bayesian variable selection}

\begin{frame}
  \frametitle{Spike and slab prior (II)}
  \begin{itemize}
  \item In this section, we follow \citep{George1997}.
  \item The prior in this method is called a \alert{spike and slab
      prior}.
  \item In Bayesian variable selection (BVS), $p$ indicator
    variables $\bm\gamma = (\gamma_1, \ldots, \gamma_p)'$ are added to 
    the model, indicating that $\beta_i$ is included in the model if
    $\gamma_i=1$ and excluded if $\gamma_i=0$.
  \item A conjugate prior is given by
    \begin{equation*}
      \pi(\bm\beta, \sigma, \bm\gamma) = \pi(\bm\beta|\sigma, \bm\gamma)\pi
      (\sigma|\bm\gamma) \pi(\bm\gamma),
    \end{equation*}
    with
    \begin{equation*}
      \bm\beta | \sigma,\bm\gamma \sim \Ncdf(0,\sigma^2 D^\ast_\gamma
      R_\gamma D^\ast_\gamma), 
    \end{equation*}
    where $D^\ast_\gamma$ is a diagonal matrix and $R_\gamma$ is a
    correlation matrix. The specification of $D^\ast_\gamma$ is given
    below.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (II)}
  \begin{itemize}
  \item For the residual variance, using the notation of
    \citep{George1997},
    \begin{equation*}
      \sigma^2|\bm \gamma \sim \text{IG}(\nu/2, \lambda_\gamma/2). 
    \end{equation*}
    \vspace*{-\baselineskip}
  \item The parameters have the following interpretation:
    \begin{itemize}
      \addtolength{\itemsep}{2pt}
    \item $\lambda_\gamma$ is a prior estimate of $\sigma^2$,
      \item $\nu$
      is a prior ``sample size'' associated with this estimate (in
      this sense, $\nu$ quantifies the uncertainty associated with
      $\lambda_\gamma$).
    \end{itemize}
  \item Converting to the previous notation, we have $a=\nu/2$ and
    $b=\nu\cdot \lambda_\gamma/2$.
  \item \citep{George1997} recommend to choose
    $\lambda_\gamma=s^2_{\text LS}$ where $s^2_{\text LS}$ is the
    least squares estimate of $\sigma^2$ in the full model.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (II)}
  \begin{itemize}
  \item Finally, the prior of $\bm\gamma$ is conveniently modelled as
    \begin{equation}
      \label{eq:6}
      \pi(\bm \gamma) = \prod w_i^{\gamma_i} (1-w_i)^{(1-\gamma_i)}
    \end{equation}
    (although any other prior can be considered as well). If
    $w_i=\theta$, $i=1,\ldots, p$, then the model prior corresponds to
    the model prior in \eqref{eq:7}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (II)}
  \begin{itemize}
\item The $i$-th diagonal element of $D^\ast_\gamma$ is specified by  
\begin{equation*}
(D_{\gamma}^{\ast2})_{ii} =
\begin{cases}
  v_{0\gamma_{(i)}}^\ast, &\text{ if } \gamma_i=0,\\
  v_{1\gamma_{(k)}}^\ast, &\text{ if } \gamma_i=1,
\end{cases}
\end{equation*}
where $v_{0\gamma_{(i)}}^\ast$ and $ v_{1\gamma_{(k)}}^\ast$ are ``small''
and ``large'', respectively.
\item More specifically, from
\begin{equation*}
  \pi (\beta_i|\sigma, \gamma) =(1-\gamma_i) \underbrace{\Ncdf(0,\sigma^2
  v_{0\gamma_{(k)}}^\ast)}_{\text{\textcolor{red}{spike}}} + \gamma_i
\underbrace{\Ncdf (0,  \sigma^2
  v_{1\gamma_{(k)}}^\ast)}_{\text{\textcolor{red}{slab}}}, 
\end{equation*}
one can choose  $v_{0\gamma_{(k)}}^\ast$ and $ v_{1\gamma_{(k)}}^\ast$
proportional to the variance of $\hat\beta$ in the OLS estimate, i.e.,
\begin{equation*}
   v_{\cdot\gamma_{(k)}}^\ast = c_\cdot \frac{\sigma^2_{\hat\beta}}{\hat\sigma^2},
 \end{equation*}
with $c_0, c_1$ constants and where $\hat\sigma^2$ refers to an
estimate of $\sigma^2$, such as the OLS estimate of the residual
variance.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Spike and slab prior (II)}
  \begin{itemize}
\item With this specification, it is possible to calculate
 \begin{equation}
   \label{eq:5}
   \pi(\bm\gamma|\bm y) \propto g(\bm\gamma) = \left|\tilde
     {\bm X}'\tilde {\bm X}\right|^{-1/2} \left|D_\gamma^\ast R_\gamma
     D_\gamma^\ast\right|^{-1/2} (\nu \lambda +
   S_\gamma^2)^{(n+\nu)/2} \pi(\gamma),
 \end{equation}
 where
 \begin{align*}
   \tilde {\bm X} &=
                    \begin{pmatrix}
                      \bm X\\
                      (D_\gamma^\ast R_\gamma D_\gamma^\ast)^{-1/2}
                    \end{pmatrix}\\[5pt]
   \tilde{\bm Y} &=
                   \begin{pmatrix}
                     \bm Y\\
                     0
                   \end{pmatrix}\\[5pt]
   S_\gamma^2 &= \tilde{\bm Y}' \tilde{\bm Y} - \tilde{\bm Y}'
                \tilde{\bm X} ( \tilde {\bm X}' \tilde{\bm X})^{-1}
                \tilde{\bm X}' \tilde{\bm Y}. 
 \end{align*}
\end{itemize}
\end{frame}

\subsection{Connection to Ridge Regression and Lasso}

\begin{frame}
  \frametitle{Connection to ridge regression and Lasso}
  \begin{itemize}
  \item See Chapter 6 of \citep{James2013}.
  \item Let (assuming that $X$ is fixed):
    \begin{equation*}
      p(\beta|X,Y)\propto f(Y|X,\beta) p(\beta|X) = f(Y|X,\beta)p(\beta)
    \end{equation*}
    \vspace*{-\baselineskip}
  \item Assume that $p(\beta) = \prod_{j=1}^p g(\beta_j)$, with $g$ a
    density.
  \item \alert{Ridge regression}: $g$ normal distribution with mean
    zero and s.d.\ a function of the $L2$-constraint $\lambda$:
    \begin{itemize}
    \item $\Rightarrow$ Posterior mode of $\beta$ equals ridge
      regression solution
    \end{itemize}
  \item \alert{Lasso}: $g$ double-exponential (Laplace) with mean zero
    and scale parameter a function of the $L1$-constraint $\lambda$:
    \begin{itemize}
    \item $\Rightarrow$ Posterior mode or $\beta$ equals lasso
      solution 
    \end{itemize}
  \end{itemize}
\end{frame}



\subsection{VW example}


% \begin{frame}[fragile]
%   \frametitle{VW example}
% {\scriptsize%
% \begin{pyconsole}[vw][frame=single]
% from python import BVS
% # BVS is my class for Bayesian Variable Selection

% bm = BVS.BayesModel(np.array(Y), np.array(X), m, 1 * M, a, b)


% # The call below produces an overflow error; this happens a lot as
% # often very large and very small constants are involved
% bm.marginal_likelihood()
% \end{pyconsole}
% }
% \end{frame}

\begin{frame}[fragile]
  \frametitle{VW example}
  \begin{itemize}
  \item Marginal likelihood via MCMC:
  \end{itemize}
{\scriptsize%
\begin{pyconsole}[vw][frame=single]
# simulate marginal likelihood via MCMC
x, sc = bm.simulate_posterior_probability(nsimulations=5000, rel=True, \
         theta=8/34)
\end{pyconsole}
}

\begin{itemize}
\item Gamma posterior from \eqref{eq:5}: 
\end{itemize}
{\scriptsize%
\begin{pyconsole}[vw][frame=single]
v0 = 0.001 * result.bse / result.resid.std()
v1 = 1 * result.bse /result.resid.std()
v0.index=range(p)
v1.index=range(p)
x2, sc2 = bm.simulate_gamma_posterior(nu, lam, v0, v1, theta=8/34, \
         nsimulations=3000, rel=True)
\end{pyconsole}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{VW example}
{\scriptsize%
\begin{pyconsole}[vw][frame=single]
res = None
for k in range(p):
    if k==0:
        res = pd.DataFrame([[X.columns[k], \
                         x[np.where(sc[:,k]==1)[0]].shape[0]/x.shape[0], \
                         x2[np.where(sc2[:,k]==1)[0]].shape[0]/x2.shape[0], \
                         result.pvalues.iloc[k]]])
    else:
        res = res.append([[X.columns[k], \
                         x[np.where(sc[:,k]==1)[0]].shape[0]/x.shape[0],\
                         x2[np.where(sc2[:,k]==1)[0]].shape[0]/x2.shape[0], \
                         result.pvalues.iloc[k]]])

res.index = range(p)
res.columns = ['coef', 'PIP', 'BVS', 'pvalue']
\end{pyconsole}
}
\end{frame}

\begin{frame}
  \frametitle{VW example}
  \begin{itemize}
  \item The \alert{median probability model} selects those variables
    that have probability greater than $0.5$.
  \item \citep{Barbieri2004} show that the median probability model is
    often optimal in terms of prediction.
  \item (Note that the median probability model depends on the choice
    of $\theta$, which specifies the probability of including /
    excluding a variable)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{VW example}
{\footnotesize%
\begin{pyconsole}[vw][frame=single]
print(res[res['PIP']>0.5].round(4))
\end{pyconsole}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{VW example}
{\footnotesize%
\begin{pyconsole}[vw][frame=single]
print(res[res['BVS']>0.5].round(4))
\end{pyconsole}
}
\end{frame}

\begin{frame}
  \frametitle{VW example}
  \begin{itemize}
  \item In application, we estimated PIP's for several hundreds of
    stocks and across a rolling time window.
  \item To ensure some minimal stability, prior knowledge about
    country and industry is included (e.g.\ Consumer
    Discretionary and Germany are always included for VW).
  \item This kind of ``fine-tuning'' is not possible with Lasso. 
  \end{itemize}
\end{frame}

\begin{frame}%[allowframebreaks]
  \frametitle{References} 
    \bibliographystyle{plainnat}
    \bibliography{finance} %
\end{frame}


\section*{}
\begin{frame}
  \frametitle{$\left.\right.$}
  \begin{center}
    \vspace{1.5cm}
    \large{\bf Thank you!}
    \vspace{2.25cm}
  \end{center}
  \begin{columns}[b]
    \column{.7\linewidth} \scalebox{.75}{ %
      \begin{minipage}{1.2\linewidth}
        {\bf Prof.\ Dr.\ Natalie Packham}\\
        Professor of Mathematics and Statistics\\
        Berlin School of Economics and Law\\
	Badensche Str.\ 52\\
        10825 Berlin
      \end{minipage}
    } %
    \vspace{0pt} \column{.3\linewidth}
    \includegraphics[width=3cm]{HWR_Logo_RGB.jpg}
    \vspace{0pt}
  \end{columns}
\end{frame}
\end{document}

