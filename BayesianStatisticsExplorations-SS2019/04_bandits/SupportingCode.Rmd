---
title: "Bandits"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 0)
library(pwr)
library(bayesAB)
#library(ggpubr)
```


## Power computations

```{r}
pwr.norm.test(d=0.25, sig.level=0.05, power=0.9)$n

pwr.norm.test(d=0.25, sig.level=0.05, power=0.95)$n

pwr.norm.test(d=0.5, sig.level=0.05, power=0.95)$n
```
```{r,echo=FALSE}
options(digits = 7)
```

```{r}
power.prop.test(p1=0.04,p2=0.05,sig.level = 0.05,power = 0.95)
```

```{r}
power.prop.test(p1=0.04,p2=0.08,sig.level = 0.05,power = 0.95)
```


## Bayes AB testing

```{r, eval=TRUE, fig.height= 4, fig.width=5}
library(bayesAB)

A_binom <- rbinom(250, 1, .25)
B_binom <- rbinom(250, 1, .2)

AB1 <- bayesTest(A_binom, B_binom, priors = c('alpha' = 65, 'beta' = 200), distribution = 'bernoulli')
plot(AB1)
```

Uniform prior:

```{r, fig.height= 4, fig.width=5}
A_binom <- rbinom(100, 1, .55)
B_binom <- rbinom(100, 1, .5)

# Fit bernoulli test
AB1 <- bayesTest(A_binom,
                 B_binom,
                 priors = c('alpha' = 1, 'beta' = 1),
                 distribution = 'bernoulli')

plot(AB1)
summary(AB1)
```
## Optional Stopping

Suppose we are running an A/B test on a new feature. We run the experiment for 20 days, and on each day get about 10,000 impressions. We use the Chi-squared test for the difference between two proportions to test whether the new feature improved click-through rate. Unbeknownst to us, the new feature has absolutely no effect- the clickthrough rate is always .1%.

```{r cache = FALSE}
library(ggplot2)
theme_set(theme_bw())
```

```{r simulation_setup,eval=FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)

# the custom splittestr package, found at
# 
#library(splittestr)
#for(f in list.files("extraCode/",full.names = TRUE)) print(f)
for(f in list.files("extraCode/", full.names = TRUE)) source(f)
```

```{r pvalue_simulation,eval=FALSE}
nreps <- 5000

set.seed(2015 - 08 - 07)
sim_no_difference <- data_frame(replicate = seq_len(nreps)) %>%
  mutate(proportion_A = .001,
         effect = 0,
         per_day = 10000) %>%
  perform_simulation(approx = TRUE)

# add chi-squared p-values
sim_no_difference <- sim_no_difference %>%
  mutate(p.value = vectorized_prop_test(sA, nA - sA, sB, nB - sB))
```

```{r process_pvalues, dependson = "pvalue_simulation",eval=FALSE}
last_day_pvals <- sim_no_difference %>%
  filter(day == 20) %>%
  .$p.value
```

```{r plot_paths_function,eval=FALSE}
plot_paths <- function(dat, color_by = "pass_end", hline = .05,
                       yaxis = "p.value",
                       labels = c("Not Significant", "Significant")) {
  dat %>%
    ggplot(aes_string("day", yaxis, group = "replicate")) +
    geom_line(aes_string(alpha = color_by, color = color_by)) +
    geom_hline(color = "red", yintercept = hline, lty = 2) +
    scale_color_manual(values = c("black", "red"), labels = labels) +
    scale_alpha_manual(values = c(.15, 1), labels = labels) +
    labs(color = "", alpha = "") +
    xlab("Day of experiment")
}
```

If we look at the results on the 20th day, we find that 5 of our simulations fell below our p-value cutoff of .05. The rate at which we falsely call nulls significant is called the [type I error rate](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). By setting a p-value threshold of .05, we set a goal that that error rate should be 5%, and we (roughly) kept that promise. The system worked!

**But!** Suppose you recorded your p-values at the end of each day. Among a hundred experiments, you might see something like this:

```{r plot_paths_pvalue, dependson = "plot_paths_function",eval=FALSE}
alpha <- .05

set.seed(3)
subsim_pvalue <- sim_no_difference %>%
  filter(replicate %in% sample(nreps, 100)) %>%
  group_by(replicate) %>%
  mutate(pass_end = last(p.value) < alpha) %>%
  mutate(pass_anywhere = min(p.value) < alpha)

lastdaysig <- subsim_pvalue %>%
  filter(day == 20, p.value < alpha)

plot_paths(subsim_pvalue, hline = alpha) +
  geom_point(color = "red", data = lastdaysig) +
  ylab("P-value")
```

Notice that over time, each experiment's p-value squiggled up and down before ending where it did at day 20. There are still only 5 experiments that *ended* with a p-value below .05 (highlighted in red). But notice that the experiment often "dips into significance" before coming back out. What if we were impatient, and stopped those experiment right then?

```{r plot_paths_pvalue_stopping, dependson = "plot_paths_pvalue",eval=FALSE}
anywhere_sig <- subsim_pvalue %>%
  filter(p.value < .05) %>%
  slice(1)

subsim_pvalue %>%
  filter(head(cumsum(c(1, p.value) < .05), -1) == 0) %>%
  plot_paths(color_by = "pass_anywhere", hline = alpha) +
  geom_point(color = "red", data = anywhere_sig) +
  ylab("P-value")

min_pvalues <- sim_no_difference %>%
  group_by(replicate) %>%
  summarize(min = min(p.value)) %>%
  .$min
```

Even though none of the simulations had a real effect, 15 of them drop below the significance cutoff of .05 at some point. If we stopped the experiment right then, we would end up calling them significant- and our Type I error rate goes way up. Our method didn't keep its promise!

## Regret

Let $y_t =(y_1, \ldots ,y_t)$ denote the sequence of rewards observed up to time $t$. Let $a_t$ denote the arm of the bandit that was played at time $t$ and $\mu_a(\theta)$ denote the expected reward.

Let $\mu^*(\theta)=max_a\{\mu_a(\theta)\}$, the expected reward under the truly optimal arm, and let $n_{at}$ at denote the number of observations that were
allocated to arm $a$ at time $t$. Then the expected cumulative regret is
 
$$
L = \sum_{t=1}^T{\sum_{a}{n_{at} \left( \mu^*(\theta) - \mu_a(\theta) \right) }}
$$